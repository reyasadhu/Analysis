batch_size: 128
dropout: 0.3
learning_rate: 2e-5
hidden_layers: 2
epochs: 3
max_batch_size_sentiment: 32
max_batch_size_sarcasm: 16
max_batch_size_negation: 32
max_batch_size_similarity: 32
grad_clip: 1.0
